{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler,OrdinalEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "conf = [(datetime(2020, 3, 17), datetime(2020, 5, 10)), (datetime(2020, 10, 30), datetime(2020, 11, 27)), (datetime(2021, 4, 3), datetime(2021, 5, 2))]\n",
    "conf_a = [(datetime(2020, 5, 11), datetime(2020, 6, 1)), (datetime(2020, 11, 28), datetime(2020, 12, 15))]\n",
    "c_v = [(datetime(2020, 12, 16), datetime(2021, 4, 2)), (datetime(2021, 5, 3), datetime(2021, 6, 20))]\n",
    "\n",
    "def is_in_range(date):\n",
    "    for start, end in conf:\n",
    "        if start <= date <= end:\n",
    "            return 3\n",
    "    for start, end in conf_a:\n",
    "        if start <= date <= end:\n",
    "            return 2\n",
    "    for start, end in c_v:\n",
    "        if start <= date <= end:\n",
    "            return 1\n",
    "    if date == datetime(2020, 12, 25):\n",
    "        return 0\n",
    "    return 0\n",
    "\n",
    "def apply_confinement_status(X):\n",
    "    X = X.copy()  # To avoid modifying the original DataFrame\n",
    "    X['Confinement'] = X['date'].apply(is_in_range)\n",
    "    return X\n",
    "\n",
    "\n",
    "def _encode_dates(X):\n",
    "    X = X.copy()\n",
    "    X['date'] = pd.to_datetime(X['date'])\n",
    "    X['year'] = X['date'].dt.year\n",
    "    X['month'] = X['date'].dt.month\n",
    "    X['day'] = X['date'].dt.day\n",
    "    #X['weekday'] = X['date'].dt.weekday\n",
    "    X['hour'] = X['date'].dt.hour\n",
    "    return X\n",
    "\n",
    "def encode_week_end(X):\n",
    "    # Assuming X is a DataFrame with a 'date' column\n",
    "    # The 'date' column should be in datetime format\n",
    "    X = X.copy()  # To avoid modifying the original DataFrame\n",
    "    X['Week_day'] = X['date'].dt.dayofweek.apply(lambda x: 'Week_day' if x in range(0, 5) else 'Weekend')\n",
    "    return X\n",
    "\n",
    "def encode_season(X):\n",
    "   \n",
    "    # Assuming X is a DataFrame with a 'month' column\n",
    "    X = X.copy()  # To avoid modifying the original DataFrame\n",
    "    X['season'] = X['month'].apply(lambda x: 'Winter' if x in [12, 1, 2] \n",
    "                                          else 'Spring' if x in [3, 4, 5] \n",
    "                                          else 'Summer' if x in [6, 7, 8] \n",
    "                                          else 'Autumn')\n",
    "    \n",
    "    return X\n",
    "\n",
    "def split_address_and_map_direction(X):\n",
    "    # Split the 'counter_name' into two columns 'Address' and 'Direction'\n",
    "    X[['Address', 'Direction']] = X['counter_name'].str.rsplit(' ', n=1, expand=True)\n",
    "\n",
    "    # Define the direction mapping\n",
    "    direction_mapping = {\n",
    "        'E-O': 1, 'O-E': -1, 'NO-SE': 1, 'SE-NO': -1, 'SO-NE': 1, 'SE-NO': -1, 'N-S': 1, 'S-N': -1,\n",
    "    }\n",
    "\n",
    "    # Apply the mapping to the 'Direction' column\n",
    "    X['Direction'] = X['Direction'].map(direction_mapping)\n",
    "    X['Direction'].fillna(1, inplace=True)\n",
    "\n",
    "    return X\n",
    "\n",
    "def calculate_time_since_installation(X):\n",
    "    X = X.copy()  # To avoid modifying the original DataFrame\n",
    "    X['counter_installation_date'] = pd.to_datetime(X['counter_installation_date'])\n",
    "    X['Time_since_installation'] = X['date'].dt.date - X['counter_installation_date'].dt.date\n",
    "    X['Time_since_installation'] = X['Time_since_installation'].apply(lambda x: x.days)\n",
    "    return X\n",
    "\n",
    "\n",
    "def get_elevation_for_dataframe(X):\n",
    "    # Define the function to get elevation\n",
    "    def get_elevation(lat, lon):\n",
    "        url = f\"https://api.open-elevation.com/api/v1/lookup?locations={lat},{lon}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            elevation = response.json()['results'][0]['elevation']\n",
    "            return elevation\n",
    "        else:\n",
    "            return None  # Return None or a default value if the API call fails\n",
    "\n",
    "    # Get unique latitude-longitude pairs\n",
    "    unique_lat_lon = X[['latitude', 'longitude']].drop_duplicates()\n",
    "\n",
    "    # Get elevation for each unique pair\n",
    "    unique_lat_lon['elevation'] = unique_lat_lon.apply(lambda row: get_elevation(row['latitude'], row['longitude']), axis=1)\n",
    "\n",
    "    # Create a dictionary for mapping\n",
    "    elevation_dict = dict(zip(unique_lat_lon['latitude'], unique_lat_lon['elevation']))\n",
    "\n",
    "    # Map the elevation to the DataFrame\n",
    "    X['Elevation'] = X['latitude'].map(elevation_dict)\n",
    "\n",
    "    return X\n",
    "\n",
    "def column_to_drop(X):\n",
    "    #return X.drop(['date','longitude','latitude','counter_technical_id','counter_id','site_id','counter_installation_date','coordinates', 'average_wind_direction' , 'average_wind_speed','horizontal_visibility','humidity','Temperature'],axis=1)\n",
    "    return X.drop(['date','longitude','latitude','counter_technical_id','counter_id','site_id','counter_installation_date','coordinates','counter_name','site_name'],axis=1)\n",
    "\n",
    "def comprehensive_preprocessing(X):\n",
    "    X = _encode_dates(X)\n",
    "    X = encode_week_end(X)\n",
    "    X = encode_season(X)\n",
    "    X=apply_confinement_status(X)\n",
    "    X=split_address_and_map_direction(X)\n",
    "    X=calculate_time_since_installation(X)\n",
    "    X=get_elevation_for_dataframe(X)\n",
    "    X = column_to_drop(X)\n",
    "   \n",
    "    print(X.info())\n",
    "   \n",
    "    return X\n",
    "\n",
    "\n",
    "\n",
    "class DataCaptureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # nothing to fit\n",
    "\n",
    "    def transform(self, X):\n",
    "        self.data = X.copy()  # store the data\n",
    "        return X  # pass the data along unchanged\n",
    "\n",
    "\n",
    "def get_estimator():\n",
    "# Now use this function in FunctionTransformer\n",
    "    \n",
    "    data_encoder = FunctionTransformer(comprehensive_preprocessing)\n",
    "\n",
    "    categorical_encoder = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    categorical_cols = ['season','Address','Week_day']\n",
    "    \n",
    "    standardize_cols = ['Temperature','average_wind_speed']\n",
    "    #['t', 'dd' , 'ff','vv','u','Elevation','Time_since_installation','log_bike_count_-1', 'log_bike_count_-2']\n",
    "    standardizer = StandardScaler()\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "\n",
    "        [(\"cat\", categorical_encoder, categorical_cols),\n",
    "         \n",
    "          (\"stand\", standardizer, standardize_cols)\n",
    "         \n",
    "        ],\n",
    "    remainder='passthrough'\n",
    "\n",
    "    )\n",
    "\n",
    "    #regressor =RandomForestRegressor()\n",
    "    regressor = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)\n",
    "    \n",
    "    data_capture_step = DataCaptureTransformer()\n",
    "\n",
    " \n",
    "\n",
    "    pipe = make_pipeline(data_encoder,preprocessor,data_capture_step , regressor)\n",
    "\n",
    "    return pipe\n",
    "\n",
    "df_train =pd.read_parquet(\"../input/mdsb-2023/train.parquet\")\n",
    "df_test = pd.read_parquet(\"../input/mdsb-2023/final_test.parquet\")\n",
    "weather=pd.read_csv(\"../input/mdsb-2023/external_data.csv\",sep='or|,+', engine='python')\n",
    "weather=weather[['date','t','dd','ff','vv','u']]\n",
    "new_name= {'t' : 'Temperature','dd' : 'average_wind_direction' , 'ff' : 'average_wind_speed','vv' : 'horizontal_visibility' , 'u' : 'humidity'}\n",
    "weather.rename(columns=new_name,inplace=True)\n",
    "\n",
    "\n",
    "## ADD MERGE OF PONT\n",
    "\n",
    "\n",
    "weather['date']=pd.to_datetime(weather['date'])\n",
    "# Define your date range\n",
    "start_date = datetime.strptime('2020-09-01 01:00:00', \"%Y-%m-%d %H:%M:%S\")\n",
    "end_date = datetime.strptime('2021-09-09 23:00:00', \"%Y-%m-%d %H:%M:%S\")\n",
    "# Filter the DataFrame\n",
    "weather_train = weather[(start_date <= weather['date']) & (weather['date'] <= end_date)]\n",
    "weather_train = weather_train.copy()\n",
    "weather_train.drop(2018, inplace=True)\n",
    "weather_train = weather_train.set_index('date')\n",
    "# Resample to hourly and forward fill the missing values\n",
    "df_hourly = weather_train.resample('H').ffill()\n",
    "# Reset index if you want 'date' back as a column\n",
    "weather_hourly = df_hourly.reset_index()\n",
    "\n",
    "df_train.reset_index(inplace=True)\n",
    "\n",
    "df_train = pd.merge(df_train,weather_hourly,on='date' ) \n",
    "\n",
    "\n",
    "df_train.set_index('index',inplace=True)\n",
    "\n",
    "df_train= df_train.sort_index()\n",
    "\n",
    "# Extract features and target\n",
    "\n",
    "X_train = df_train.drop(['log_bike_count','bike_count'],axis=1)\n",
    "y_train = df_train['log_bike_count']\n",
    "\n",
    "pipeline = get_estimator()\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "df_test = pd.read_parquet(\"../input/mdsb-2023/final_test.parquet\")\n",
    "\n",
    "pd.to_datetime(df_test['date'])\n",
    "\n",
    "start_date = min(df_test['date'])\n",
    "end_date = max(df_test['date'])\n",
    "# Filter the DataFrame\n",
    "weather_test = weather[(start_date <= weather['date']) & (weather['date'] <= end_date)]\n",
    "\n",
    "weather_test = weather_test.set_index('date')\n",
    "row_to_copy = weather_test.loc['2021-09-10 03:00:00']\n",
    "\n",
    "# Create new rows with the same values but different indices\n",
    "new_rows = pd.DataFrame([row_to_copy, row_to_copy], \n",
    "                        index=pd.to_datetime(['2021-09-10 02:00:00', '2021-09-10 01:00:00']))\n",
    "\n",
    "weather_test = pd.concat([weather_test, new_rows])\n",
    "\n",
    "# Sort the DataFrame by index\n",
    "weather_test.sort_index(inplace=True)\n",
    "# Resample to hourly and forward fill the missing values\n",
    "df_hourly = weather_test.resample('H').ffill()\n",
    "# Reset index if you want 'date' back as a column\n",
    "\n",
    "weather_hourly = df_hourly.reset_index().rename(columns={'index': 'date'})\n",
    "\n",
    "df_test.reset_index(inplace=True)\n",
    "\n",
    "print(df_test[df_test['date']==datetime.strptime('2021-09-12 14:00:00', \"%Y-%m-%d %H:%M:%S\")].index)\n",
    "\n",
    "df_test= pd.merge(df_test,weather_hourly,on='date' ) \n",
    "\n",
    "df_test.set_index('index',inplace=True)\n",
    "df_test= df_test.sort_index()\n",
    "\n",
    "print(df_test[df_test['date']==datetime.strptime('2021-09-12 14:00:00', \"%Y-%m-%d %H:%M:%S\")].index)\n",
    "X_test = df_test\n",
    "\n",
    "\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    " \n",
    "\n",
    "# Create submission file\n",
    "\n",
    "results = pd.DataFrame({\n",
    "\n",
    "    'Id': np.arange(y_pred.shape[0]),\n",
    "\n",
    "   'log_bike_count': y_pred\n",
    "\n",
    "})\n",
    "\n",
    "results.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
